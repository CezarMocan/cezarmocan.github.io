<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-10-09T19:01:20-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">cezar.io</title><subtitle>Personal website.</subtitle><entry><title type="html">[Performative Avatars] 3D Scanning</title><link href="http://localhost:4000/2020/10/12/3d-scan.html" rel="alternate" type="text/html" title="[Performative Avatars] 3D Scanning" /><published>2020-10-12T01:01:00-04:00</published><updated>2020-10-12T01:01:00-04:00</updated><id>http://localhost:4000/2020/10/12/3d-scan</id><content type="html" xml:base="http://localhost:4000/2020/10/12/3d-scan.html">&lt;p&gt;Doing a 3D scan proved to be a lot of fun, my partner and I scanned each other a few times (and also considered scanning Lentil the cat, but quickly gave up on that :) ). However the results did not turn out great, mostly due to my being stubborn in deciding to use a Microsoft Kinect 2 (the XBox One version) I own, instead of a structure sensor from the ER.&lt;/p&gt;

&lt;p&gt;It turns out the Skanect software developers had decided not to implement support for this version of the Kinect, due to its poor results in scanning. I ended up using the default Windows 3D Scan app, which worked well. My scan’s mesh turned out fine – low-ish resolution, but not bad overall, and it can probably be smoothed out a bit in Maya. However, the texture has a large amount of visual glitches all over the place, aside from the poor image quality (which I was expecting.)&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-12-3d-scan/1.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-12-3d-scan/2.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-12-3d-scan/3.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;figcaption&gt;
    3D Scan Images
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-12-3d-scan/4.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-12-3d-scan/5.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-12-3d-scan/6.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;figcaption&gt;
    3D Scan Images, and details of the texture problems
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I’m not quite sure whether the texture quality is a matter of hardware or software, but I assume it’s something I could entirely avoid by using equipment from the ER. I might do that this coming week, in order to work with a higher quality scan of myself.&lt;/p&gt;

&lt;p&gt;However, I find the texture file (glitchy as it is) to be quite gorgeous!&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-12-3d-scan/texture.png&quot; alt=&quot;texture&quot; /&gt;
  &lt;figcaption&gt;
    Texture file
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">Doing a 3D scan proved to be a lot of fun, my partner and I scanned each other a few times (and also considered scanning Lentil the cat, but quickly gave up on that :) ). However the results did not turn out great, mostly due to my being stubborn in deciding to use a Microsoft Kinect 2 (the XBox One version) I own, instead of a structure sensor from the ER.</summary></entry><entry><title type="html">[Performative Avatars] Head scan and wrapping</title><link href="http://localhost:4000/2020/10/12/wrap3.html" rel="alternate" type="text/html" title="[Performative Avatars] Head scan and wrapping" /><published>2020-10-12T01:01:00-04:00</published><updated>2020-10-12T01:01:00-04:00</updated><id>http://localhost:4000/2020/10/12/wrap3</id><content type="html" xml:base="http://localhost:4000/2020/10/12/wrap3.html">&lt;p&gt;I ended up starting this week’s assignment by re-doing my 3D scan, since the one I had done with a Kinect v2 last week turned out quite poor – decent mesh, but horrible textures. Bellus3D turned out much better, the texture resolution is high and the mesh has a lot of detail. There are still a few glitches – the texture on the nose looks stretched, there is no gap behind the ears, and most visibly the texture on the top &amp;amp; back of the head is inferred, rather than captured (the app only scans the front &amp;amp; side of your head, not the back.)&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-19-wrap3/doc-7.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-19-wrap3/doc-8.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-19-wrap3/doc-9.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;figcaption&gt;
    Head 3D Scans done using the Bellus3D iOS app.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I then followed the Wrap3 tutorial in order to clean up my scan and attach it to a body. I didn’t realize Matt had sent the .wrap files over email, so I built the little script based on the video tutorial, but it was pretty easy.&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-19-wrap3/doc-1.png&quot; alt=&quot;Wrap3&quot; /&gt;
  &lt;figcaption&gt;
    Screenshot from Wrap3, after the entire process was complete.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I tried attaching my head to a Daz body (and to a Fuse body) in Maya. While combining the meshes – with some imperfections around the neck – wasn’t difficult, I don’t know Maya well enough to merge the textures &amp;amp; update the UVs. In theory I know I should combine the textures into a single file, add it to the scene, and move around the UV maps to match the single texture, but I didn’t manage to do it. I am also not sure whether this process would’ve produced good results when importing the new mesh in Wrap3…&lt;/p&gt;

&lt;p&gt;As a next step, I did the mesh scaling, rotation and alignment in Maya.&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-19-wrap3/doc-3.png&quot; alt=&quot;Mesh in Maya&quot; /&gt;
  &lt;figcaption&gt;
    Mesh in Maya.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I also exported a meshes from Wrap3 with a few different settings (compression parameter, export normals toggle.)&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-19-wrap3/doc-6.png&quot; alt=&quot;Mesh in Maya&quot; /&gt;
  &lt;figcaption&gt;
    3 different meshes in Maya, exported with different options from Wrap3.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;At the end, I uploaded my &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;obj&lt;/code&gt; file to Mixamo, to make sure it can be rigged. It worked well. I also brought it into Unreal and applied the texture, for testing purposes, but didn’t document that part.&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-19-wrap3/doc-4.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-10-19-wrap3/doc-5.png&quot; alt=&quot;3D Scan&quot; /&gt;
  &lt;figcaption&gt;
    My scan (with Wrap&apos;s default body) in Mixamo.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The big open question I have after this assignment is how can I attach my head to the body of an existing avatar? Also, Wrap3 seemd to decrease the mesh resolution of my head scan even at the lowest compression setting, but I assume it’s still high-res enough.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">I ended up starting this week’s assignment by re-doing my 3D scan, since the one I had done with a Kinect v2 last week turned out quite poor – decent mesh, but horrible textures. Bellus3D turned out much better, the texture resolution is high and the mesh has a lot of detail. There are still a few glitches – the texture on the nose looks stretched, there is no gap behind the ears, and most visibly the texture on the top &amp;amp; back of the head is inferred, rather than captured (the app only scans the front &amp;amp; side of your head, not the back.)</summary></entry><entry><title type="html">[Performative Avatars] Sequencer</title><link href="http://localhost:4000/2020/10/05/sequencer.html" rel="alternate" type="text/html" title="[Performative Avatars] Sequencer" /><published>2020-10-05T01:01:00-04:00</published><updated>2020-10-05T01:01:00-04:00</updated><id>http://localhost:4000/2020/10/05/sequencer</id><content type="html" xml:base="http://localhost:4000/2020/10/05/sequencer.html">&lt;p&gt;I continued working with my Daz character, and took last week’s investigations around repetition, body parts, textures and close-ups one step further. At the same time, I’ve wanted to make a digital character cry for some months now, so I spent a good amount of time figuring out how to apply tears on the avatar’s skin. I had this very specific animated &amp;amp; over the top tear in mind from a Mika Rottenberg piece, but I can’t find it anymore… I think the tears I ended up making are somewhat close to what I was going for, 70% realistic and 30% obviously over the top, but they could use a lot of work. The whole video is going all in and beyond on the d33p sadness of the track I chose &amp;amp; of the character. I ended up shooting individual scenes in the Sequencer, and editing them together (with sound as well) in Premiere.&lt;/p&gt;
&lt;div class=&quot;mb2&quot; style=&quot;padding:50% 0 0 0;position:relative;&quot;&gt;
  &lt;iframe src=&quot;https://player.vimeo.com/video/465232452/&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">I continued working with my Daz character, and took last week’s investigations around repetition, body parts, textures and close-ups one step further. At the same time, I’ve wanted to make a digital character cry for some months now, so I spent a good amount of time figuring out how to apply tears on the avatar’s skin. I had this very specific animated &amp;amp; over the top tear in mind from a Mika Rottenberg piece, but I can’t find it anymore… I think the tears I ended up making are somewhat close to what I was going for, 70% realistic and 30% obviously over the top, but they could use a lot of work. The whole video is going all in and beyond on the d33p sadness of the track I chose &amp;amp; of the character. I ended up shooting individual scenes in the Sequencer, and editing them together (with sound as well) in Premiere.</summary></entry><entry><title type="html">[Performative Avatars] Animations</title><link href="http://localhost:4000/2020/09/28/animation.html" rel="alternate" type="text/html" title="[Performative Avatars] Animations" /><published>2020-09-28T01:01:00-04:00</published><updated>2020-09-28T01:01:00-04:00</updated><id>http://localhost:4000/2020/09/28/animation</id><content type="html" xml:base="http://localhost:4000/2020/09/28/animation.html">&lt;iframe width=&quot;760&quot; height=&quot;430&quot; src=&quot;https://www.youtube.com/embed/leW_4ARmVAY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;For this week’s assignment, I continued working with the Daz avatar I had created during the first week. I ended up practicing the workflow I discovered last week around mapping a Mixamo animation onto a Daz character’s skeleton, by bringing in 3 more animations for my character into Unreal.&lt;/p&gt;

&lt;p&gt;Instead of using the pre-existing Unreal third person character, I set up mine to start by being still, and gradually increase its speed (which is connected to a blend space with 3 animations: still, walking, running) over the course of a few minutes. I’ve also added a key event for the character to jump when I press ‘Q’.&lt;/p&gt;

&lt;p&gt;In addition to last week’s single camera view, I’ve expanded the scene to include 3 cameras which show close-ups of different body parts or angles of the character, and as well as a wide shot, all following the avatar as it’s walking. I’m beginning to find close-ups fascinating in this context, as they reveal how artificial everything is in an almost innocent manner. This especially applies to the foot (whose skeleton is still slightly wrongly mapped.)&lt;/p&gt;

&lt;p&gt;I used Bolero as a soundtrack for the character’s journey to nowhere, as it’s a highly repetitive composition which I found matches the way my avatar is stuck in a loop with the four animations it knows how to do. I live-performed changing the cameras and making my character jump to the sound, trying to create a bit of a sense of rhythm. The video is definitely half-baked. First of all, the environment is almost empty, so it’s almost impossible to tell that the character is moving through space. The wide shot doesn’t really work, and I think I should have significantly more close-ups of different body parts to loop through. But there might be some things in it that I could see taking further.&lt;/p&gt;

&lt;p&gt;My main question for the week is how I could edit with sound in the Sequencer, but I think the answer might just be a Google search away :).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">For this week’s assignment, I continued working with the Daz avatar I had created during the first week. I ended up practicing the workflow I discovered last week around mapping a Mixamo animation onto a Daz character’s skeleton, by bringing in 3 more animations for my character into Unreal.</summary></entry><entry><title type="html">[Performative Avatars] Skeletal Mesh</title><link href="http://localhost:4000/2020/09/22/skeletal-mesh.html" rel="alternate" type="text/html" title="[Performative Avatars] Skeletal Mesh" /><published>2020-09-22T01:01:00-04:00</published><updated>2020-09-22T01:01:00-04:00</updated><id>http://localhost:4000/2020/09/22/skeletal-mesh</id><content type="html" xml:base="http://localhost:4000/2020/09/22/skeletal-mesh.html">&lt;figure style=&quot;margin: 0; margin-bottom: 20px;&quot;&gt;
    &lt;video class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-22-skeletal-mesh/running.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
      &lt;figcaption&gt;
    Skeletal Mesh running around in my environment
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;For this week’s assignment, I revisited my understanding of skeletal meshes and working with them in Unreal Engine, through Matt’s tutorials. I became familiar with this type of mesh last semester, through work I did for Synthetic Architectures and for my Live Image Processing &amp;amp; Performance final project.&lt;/p&gt;

&lt;p&gt;The Unreal concepts are clear to me so far, I’m finally starting to feel a little bit of control over the software. My personal learning goal for this week however has been figuring out how to bring an avatar created in Daz Studio into Unreal, and applying a Mixamo animation to it. Bringing an FBX file exported from Daz into the Mixamo web interface doesn’t work properly (conversion fails, for some reason…,) so I had to look for alternatives. I found a &lt;a href=&quot;https://www.youtube.com/watch?v=pEK7fvAIV_A&quot;&gt;Youtube tutorial&lt;/a&gt; which explains how to use Maya as a bridge, in order to apply an animation downloaded from Mixamo to a custom Maya character – which in my case was a Daz character, transferred to Maya using the Daz to Maya bridge. This process involves creating a control rig for the downloaded Mixamo character, whose bone mapping needs to 100% match the control rig of the exported Daz character. I accidentally mis-mapped some of the hand bones (image below), which caused my character’s fingers to look completely broken, but other than that, the tutorial got me what I needed.&lt;/p&gt;

&lt;figure style=&quot;margin: 0; margin-bottom: 20px;&quot;&gt;
    &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-22-skeletal-mesh/hands.png&quot; /&gt;
  &lt;figcaption&gt;
    Twisted fingers because of hand skeleton wrong mapping in Maya
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The other thing I wanted to learn more about this week had to do with the textures associated with my character, so I gave him a few face tattoos (“Trust your body” / “Trust nobody”) by adding the text in Photoshop on the texture file associated with the body material.&lt;/p&gt;

&lt;p&gt;In the video above, I also used a camera shake in order to simulate the up-down running motion (camera is attached to character’s head, which can be seen in a few seconds at the beginning / end.) I also adjusted the character’s morph targets through a Blueprint. The animation would still need a good amount of work in order to feel somewhat realistic (running motion, speed, face gestures, mouth, etc.,) but I’m happy I got the Daz character to work with Mixamo animations.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">Skeletal Mesh running around in my environment For this week’s assignment, I revisited my understanding of skeletal meshes and working with them in Unreal Engine, through Matt’s tutorials. I became familiar with this type of mesh last semester, through work I did for Synthetic Architectures and for my Live Image Processing &amp;amp; Performance final project.</summary></entry><entry><title type="html">[Performative Avatars] Two Avatars</title><link href="http://localhost:4000/2020/09/14/avatar-creation.html" rel="alternate" type="text/html" title="[Performative Avatars] Two Avatars" /><published>2020-09-14T01:01:00-04:00</published><updated>2020-09-14T01:01:00-04:00</updated><id>http://localhost:4000/2020/09/14/avatar-creation</id><content type="html" xml:base="http://localhost:4000/2020/09/14/avatar-creation.html">&lt;figure style=&quot;&quot;&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/8.png&quot; alt=&quot;Daz Studio Avatar&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;I chose to build my two avatars in Second Life and Daz Studio. I had created digital versions of myself in Sims and Memoji over the summer, both of them being quite pleasant experiences. I can’t say the same about the two I made this week, even though it got me thinking about representation and about the peculiarities of my face.&lt;/p&gt;

&lt;p&gt;Choosing Second Life came out of pure curiosity – I had been aware of the platform’s existence and history for many years, and wanted to try it. It was a bad experience overall, mostly because of the software’s user interface and poor performance on my computer. The avatar creation system didn’t seem lacking in options when compared to other ones I’ve tried (plenty of options to customize the face &amp;amp; body structure,) but I had trouble creating a face that looked like mine. Strangely, the avatar height was somewhere at the top in the user interface – almost as the main defining characteristic of your image –, and for some reason I found myself caring a lot about it being accurate, 1.79m (5ft11). The hair options were incredibly limited, I had trouble finding something that made sense. The clothing system seemed to be almost entirely broken – I could not get shirt &amp;amp; pants option to load, and when I finally decided to rely on the default “Greg” pack I was given, I realized the clothes did not properly fit the mesh of my character.&lt;/p&gt;

&lt;figure style=&quot;margin: 0; margin-bottom: 20px;&quot;&gt;
    &lt;video class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/4.mp4&quot; muted=&quot;&quot; controls=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
      &lt;figcaption&gt;
    Second Life Timelapse
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/1.png&quot; alt=&quot;Second Life Avatar&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/2.png&quot; alt=&quot;Second Life Avatar&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/3.png&quot; alt=&quot;Daz Studio Avatar&quot; /&gt;
  &lt;figcaption&gt;
    Second Life Avatars
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Building the second avatar in Daz Studio was a more pragmatic choice, as I had been curious to learn the basics of the software for some time. After watching a few Youtube tutorials in order to understand the interface, I started working with the Genesis 8 Male model (gender binaries are strong in this software and community…) I quickly realized I have limited control over my avatar with the basic settings, and ended up purchasing the Face Morphs for Genesis 8 Male pack, for $11. The purchase was worth its money – I got access to hundreds of sliders, allowing me to customize bones and muscles of my face I didn’t even know I had. It’s a similar system to the Second Life one, at a significantly higher quality and fidelity.&lt;/p&gt;

&lt;p&gt;This avatar doesn’t look like me either, even though parts of it probably do (the lips, the chin, maybe the eyebrows and the nose.) If anything, it looks like an older version of myself. With so much control over the face parameters of the avatar, I had to turn on my webcam and really study my own face, as an image or object, in a way that was new and unsettling. I started obsessing about the shape, placement or angle of various parts of my face, and stopped before spiraling down a dangerous rabbit hole :).&lt;/p&gt;

&lt;figure style=&quot;margin: 0; margin-bottom: 20px;&quot;&gt;
    &lt;video class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/5.mp4&quot; muted=&quot;&quot; controls=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
      &lt;figcaption&gt;
    Daz Studio Timelapse
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/6.png&quot; alt=&quot;Daz Studio Avatar&quot; /&gt;
  &lt;figcaption&gt;
    Daz Studio Avatar
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I also tried the Face Transfer feature of the software, which allows you to upload an image of yourself, and creates an avatar based on that. The result was absolutely terrifying (granted, I did not tweak any parameters or colors in the end result. It could probably be a bit better.)&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/7.png&quot; alt=&quot;Face Transfer Avatar&quot; /&gt;
  &lt;figcaption&gt;
  Face Transfer Avatar
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">I chose to build my two avatars in Second Life and Daz Studio. I had created digital versions of myself in Sims and Memoji over the summer, both of them being quite pleasant experiences. I can’t say the same about the two I made this week, even though it got me thinking about representation and about the peculiarities of my face.</summary></entry><entry><title type="html">[Homemade Hardware] Final Project Eagle Designs</title><link href="http://localhost:4000/2020/04/17/hh-eagle-designs.html" rel="alternate" type="text/html" title="[Homemade Hardware] Final Project Eagle Designs" /><published>2020-04-17T01:01:00-04:00</published><updated>2020-04-17T01:01:00-04:00</updated><id>http://localhost:4000/2020/04/17/hh-eagle-designs</id><content type="html" xml:base="http://localhost:4000/2020/04/17/hh-eagle-designs.html">&lt;p&gt;Bill of materials can be found &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1Uv07Ad9jPWgMuVOinqs0TzJjYUghBkELgq30I0iqyWk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.
You can also download the &lt;a href=&quot;/assets/media/blog/final-board.sch&quot;&gt;Eagle Schematic&lt;/a&gt; and the &lt;a href=&quot;/assets/media/blog/final-board.brd&quot;&gt;Eagle Board&lt;/a&gt;, as well as &lt;a href=&quot;/assets/media/blog/final-circuit.pdf&quot;&gt;PDF&lt;/a&gt; &lt;a href=&quot;/assets/media/blog/final-board.pdf&quot;&gt;versions&lt;/a&gt; of the two.&lt;/p&gt;
&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/media/blog/final-shot.png&quot; alt=&quot;Board design.&quot; /&gt;
  &lt;figcaption&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Homemade Hardware" /><category term="ITP" /><summary type="html">Bill of materials can be found here. You can also download the Eagle Schematic and the Eagle Board, as well as PDF versions of the two.</summary></entry><entry><title type="html">[Critical Communications] FCC License Geo-Research</title><link href="http://localhost:4000/2020/04/12/fcc-licenses.html" rel="alternate" type="text/html" title="[Critical Communications] FCC License Geo-Research" /><published>2020-04-12T01:01:00-04:00</published><updated>2020-04-12T01:01:00-04:00</updated><id>http://localhost:4000/2020/04/12/fcc-licenses</id><content type="html" xml:base="http://localhost:4000/2020/04/12/fcc-licenses.html">&lt;p&gt;For the second Critical Communications assignment, I used &lt;a href=&quot;https://www.radioreference.com&quot;&gt;Radio Reference&lt;/a&gt; in order to search for FCC radio licenses in a few different areas and industries. Here are some of my findings.&lt;/p&gt;
&lt;figure&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/media/blog/radio-screenshot.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Search #1: My neighborhood (zipcode 11221)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I did a 1 mile radius search around my home, using the &lt;a href=&quot;https://www.radioreference.com/apps/db/?action=fccProx&amp;amp;lat=40.6947392&amp;amp;lon=-73.9270738&amp;amp;r=1&quot;&gt;lat/lon FCC Callsign search tool&lt;/a&gt; that Radio Reference offers. I got about 150 results. While many of them belong to the city (NYPD, FDNY or simply City of New York communications,) there were lots of private license holders as well.&lt;/p&gt;

&lt;p&gt;The largest category of private license holders is taxi / car service companies (e.g. &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WPAX216&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WNIM264&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WQCS994&quot;&gt;3&lt;/a&gt;, &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WQVZ377&quot;&gt;4&lt;/a&gt;). They use their licenses for radio communications between cars, the traditional way taxi drivers talk to their dispatches and to each other. However, this result took me by surprise, as I rarely see taxis in the neighborhood. Most of these FCC licenses are still active, but the companies are either very small (1-2 person,) or have been put out of business by ride sharing apps over the past few years.&lt;/p&gt;

&lt;p&gt;I also found lots of licenses for NEXTEL OF NEW YORK. I had never heard of them, but in the meanwhile I learned they are an older telco provider, which is now (part of) Sprint. (?)&lt;/p&gt;

&lt;p&gt;Another significant category of license holders is schools (mostly charter schools). I don’t fully understand how they use radio communications, but it might have to do with synchronized clocks (see notes of &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WQFN837&quot;&gt;Achievement First Bushwick Charter School&lt;/a&gt;.) Which leads me to the next discovery – &lt;a href=&quot;https://www.american-time.com/&quot;&gt;American Time&lt;/a&gt;. I had never heard of this company before – they offer synchronized time services to businesses. It’s unclear to me how this still exists when we have such widespread internet access, but I’ll let them be. One of their offerings is a &lt;a href=&quot;https://www.american-time.com/products-by-family/wall-clocks/battery-wall-clocks/radio-controlled-atomic-molded-case-clocks-fully-loaded&quot;&gt;radio controlled clock&lt;/a&gt;, which, according to the website, recives a signal over radio from an atomic clock in Colorado a few times a day in order to keep time precise. American Time holds licenses for a few thousands antennas (over 8,000 entries on their &lt;a href=&quot;https://www.radioreference.com/apps/db/?frn=0015495773&amp;amp;os=17000&amp;amp;s=ent&quot;&gt;Radio Reference page&lt;/a&gt;,) probably all used to keep their clocks all around America in sync.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Search #2: Menlo Park, Palo Alto, large tech companies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I was curious what the FCC radio license landscape is like in Silicon Valley, and how it’s different from the part of New York I live in. The radius searches in the area returned mostly 1) construction companies, 2) bio-tech companies and 3) tech companies. I started searching for company specific licenses for a few larger tech companies: Google, Facebook, Square, Palantir, Twitter, Airbnb, Pinterest, Tesla. Most of these had licenses for antennas located at their headquarters, in San Francisco / SV, or New York. Tesla also had some for their factories. Palantir didn’t have any (lol.) Google and Facebook had more than the other companies. I went deeper down the Facebook route, and realized that they have licenses for towers at the locations of their data centers as well. Which makes sense. However, the dystopian discovery that came out of this was that they got to name the streets where the data centers were built… &lt;a href=&quot;https://www.google.com/maps/place/Facebook+Data+Center/@32.9862414,-97.2594572,1279m/data=!3m1!1e3!4m13!1m7!3m6!1s0x864dd09915953d8d:0x97b577da8c062453!2s4500+Like+Way,+Fort+Worth,+TX+76177!3b1!8m2!3d32.984464!4d-97.2584655!3m4!1s0x864dd1dcb16993b1:0x1e2d97b188699b0!8m2!3d32.9867349!4d-97.2526476&quot;&gt;5000 Like Way, Fort Worth TX&lt;/a&gt; and &lt;a href=&quot;https://www.google.com/maps/place/Facebook+Data+Center/@41.6620952,-93.513984,1412m/data=!3m1!1e3!4m13!1m7!3m6!1s0x87ee917946eb868f:0x226cfeb7aacb03c!2s100+Share+Way+NW,+Altoona,+IA+50009!3b1!8m2!3d41.6632643!4d-93.5100769!3m4!1s0x87ee9176e99c041d:0x6035fff71a76b700!8m2!3d41.667416!4d-93.5076723&quot;&gt;100 Share Way NW, Altoona IA&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Search #3: Area 51 (Homey Airport)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;No results :).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using the &lt;a href=&quot;http://websdr.ewi.utwente.nl:8901/&quot;&gt;online SDR located at the Twente University&lt;/a&gt;, I caught a very clear recording of data being sent over the 26148.2kHz frequency, and an adjacent one (the one depicted in the screenshot). It looked like the two frequencies could be in dialogue, their messages never overlapped and alternated pretty consistently. Any ideas what it might be?&lt;/p&gt;

&lt;audio controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/media/blog/radio_holland_26148.2kHz.wav&quot; /&gt;
&lt;/audio&gt;

&lt;figure&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/media/blog/radio-screenshot-2.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/media/blog/radio-screenshot.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Critical Communications" /><category term="ITP" /><summary type="html">For the second Critical Communications assignment, I used Radio Reference in order to search for FCC radio licenses in a few different areas and industries. Here are some of my findings.</summary></entry><entry><title type="html">[Critical Communications] FTP</title><link href="http://localhost:4000/2020/04/05/ftp.html" rel="alternate" type="text/html" title="[Critical Communications] FTP" /><published>2020-04-05T01:01:00-04:00</published><updated>2020-04-05T01:01:00-04:00</updated><id>http://localhost:4000/2020/04/05/ftp</id><content type="html" xml:base="http://localhost:4000/2020/04/05/ftp.html">&lt;p&gt;Upon doing a round of research for communication protocols in the wild, as well as a few personal interests (&lt;em&gt;FM radio&lt;/em&gt;, which I had done a project with last semester, &lt;em&gt;telnet&lt;/em&gt; which is so old but still around or the &lt;em&gt;Spanning Tree Protocol&lt;/em&gt;, an early network routing protocol developed by a woman pioneer of computer science, Radia Perlman) I ended choosing &lt;em&gt;FTP&lt;/em&gt;, the File Transfer Protocol. I chose it mostly for nostalgic reasons. When I started using the internet in the early 2000s, I used to encounter sites served over ftp:// relatively often. Their visibility (and numbers) have been diminishing since, to the point where it took me about a reasonable amount of research in order to simply find one today. As the internet evolves, FTP will likely remain relevant for historical purposes, but its practical uses will be completely obsolete.&lt;/p&gt;

&lt;p&gt;The creation of FTP dates back to 1971. Developed by Abhay Bhushan while he was a student at MIT, the initial version of the protocol was published as &lt;a href=&quot;https://tools.ietf.org/html/rfc114&quot;&gt;IETF Request For Comments (RFC) 114&lt;/a&gt; and pre-dates TCP/IP (!!!). This proposal describes a mechanism for “indirect computer usage over a network” – an abstraction which allows a user to interact with a remote computer over the network without needing to log into the remote host, or be familiar with its command line interface. A number of RFCs improved upon the initial specification over the next few years (RFC 172, RFC 265, RFC 542, etc.,) all assuming the operation of this protocol on top of a TCP predecessor, NCP. In June 1980, after almost 10 years of existence, the protocol takes the shape of &lt;a href=&quot;https://tools.ietf.org/html/rfc765&quot;&gt;RFC 765&lt;/a&gt;, File Transfer Protocol Specification, which describes FTP communication over TCP. Another five years later, the protocol was revised and extended through &lt;a href=&quot;https://tools.ietf.org/html/rfc989&quot;&gt;RFC 959&lt;/a&gt;, which still serves as the base FTP protocol spec. Further feature and security improvements have been published since. All to say that the development of this protocol has been going hand in hand with many other low-level protocols which comprise the internet as we know it today, under the same institutional umbrella – U.S. universities or research institutions, with funding from the government or military.&lt;/p&gt;

&lt;p&gt;From a technical point of view, the protocol sits at the Application Layer of the internet protocol suite, similarly to HTTP, DNS or SSH. This means the protocol builds on top of TCP and the guarantees it offers: reliable and ordered message delivery between IP connected machines. The main goals of the protocol are data retrieval and transfer, and the commands specified in RFC 959 reflect that: RETRIEVE (RETR) and STORE (STOR) are the first ones to be listed, followed by another 20 or so commands which can manipulate the server’s file system or offer information about the service.&lt;/p&gt;

&lt;p&gt;The RFC also describes the anatomy of FTP connection between two machines as involving two communication lines: a control connection, and a data connection. The control connection is used for sending commands between the two machines (usually on port 21.) Upon establishing the control connection, the two machines agree on another set of ports for the data connection, which is where file transfers actually occur. The control connection between the two computers stays open for as long as needed, which is one of the main differences from the HTTP protocol – HTTP connections open when requests are made, and close once the data is sent, over a single line of communication.&lt;/p&gt;

&lt;p&gt;Since the design dates almost 50 years back, FTP performs file transfers in plain text, making it vulnerable from a security point of view. While the protocol specifies a mechanism for authentication, the username and password are also communicated without employing any encryption mechanisms. Secure FTP extensions have been created in the mid-late 1990s order to address the security concern, the most important ones being FTPS (FTP over SSL, just like HTTPS) and SFTP (FTP over an SSH tunnel.)&lt;/p&gt;

&lt;p&gt;Since I started going down rabbit hole in order to hopefully find some forgotten corners of the internet, I started searching for active FTP sites. I learned that FTP search engines exist and were (and are) used in similar ways to regular WWW search engines. &lt;a href=&quot;https://www.mmnt.ru/int/&quot;&gt;MAMONT&lt;/a&gt; is one that still works, and it sent me to a &lt;a href=&quot;ftp://212.85.110.14/Pub1/&quot;&gt;lovely window into 2005&lt;/a&gt; and &lt;a href=&quot;ftp://ftp.lyx.org/pub/doc/faqs/&quot;&gt;the largest collection of FAQs I have seen in my entire life&lt;/a&gt;. It made me remember the early internet browsing experience, and made me sad that major internet browsers are considering &lt;a href=&quot;https://www.pixelstech.net/article/1566007822-Google-plans-to-deprecate-FTP-URL-support-in-Chrome&quot;&gt;dropping&lt;/a&gt; &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=85464&quot;&gt;support&lt;/a&gt; for FTP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Critical Communications" /><category term="ITP" /><summary type="html">Upon doing a round of research for communication protocols in the wild, as well as a few personal interests (FM radio, which I had done a project with last semester, telnet which is so old but still around or the Spanning Tree Protocol, an early network routing protocol developed by a woman pioneer of computer science, Radia Perlman) I ended choosing FTP, the File Transfer Protocol. I chose it mostly for nostalgic reasons. When I started using the internet in the early 2000s, I used to encounter sites served over ftp:// relatively often. Their visibility (and numbers) have been diminishing since, to the point where it took me about a reasonable amount of research in order to simply find one today. As the internet evolves, FTP will likely remain relevant for historical purposes, but its practical uses will be completely obsolete.</summary></entry><entry><title type="html">[Synthetic Architectures] Final Treatment</title><link href="http://localhost:4000/2020/03/30/synth-arch-treatment.html" rel="alternate" type="text/html" title="[Synthetic Architectures] Final Treatment" /><published>2020-03-30T01:01:00-04:00</published><updated>2020-03-30T01:01:00-04:00</updated><id>http://localhost:4000/2020/03/30/synth-arch-treatment</id><content type="html" xml:base="http://localhost:4000/2020/03/30/synth-arch-treatment.html">&lt;figure&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-30-synth-arch-treatment/1.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Link to the final project treatment can be found &lt;a href=&quot;/assets/media/blog/2_Treatment_v2.pdf&quot; target=&quot;__blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="Synthetic Architectures" /><category term="ITP" /><summary type="html"></summary></entry></feed>