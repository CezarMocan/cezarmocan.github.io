<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-01T12:23:28-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">cezar.io</title><subtitle>Personal website.</subtitle><entry><title type="html">[Performative Avatars] Animations</title><link href="http://localhost:4000/2020/09/28/animation.html" rel="alternate" type="text/html" title="[Performative Avatars] Animations" /><published>2020-09-28T01:01:00-04:00</published><updated>2020-09-28T01:01:00-04:00</updated><id>http://localhost:4000/2020/09/28/animation</id><content type="html" xml:base="http://localhost:4000/2020/09/28/animation.html">&lt;iframe width=&quot;760&quot; height=&quot;430&quot; src=&quot;https://www.youtube.com/embed/leW_4ARmVAY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;For this week’s assignment, I continued working with the Daz avatar I had created during the first week. I ended up practicing the workflow I discovered last week around mapping a Mixamo animation onto a Daz character’s skeleton, by bringing in 3 more animations for my character into Unreal.&lt;/p&gt;

&lt;p&gt;Instead of using the pre-existing Unreal third person character, I set up mine to start by being still, and gradually increase its speed (which is connected to a blend space with 3 animations: still, walking, running) over the course of a few minutes. I’ve also added a key event for the character to jump when I press ‘Q’.&lt;/p&gt;

&lt;p&gt;In addition to last week’s single camera view, I’ve expanded the scene to include 3 cameras which show close-ups of different body parts or angles of the character, and as well as a wide shot, all following the avatar as it’s walking. I’m beginning to find close-ups fascinating in this context, as they reveal how artificial everything is in an almost innocent manner. This especially applies to the foot (whose skeleton is still slightly wrongly mapped.)&lt;/p&gt;

&lt;p&gt;I used Bolero as a soundtrack for the character’s journey to nowhere, as it’s a highly repetitive composition which I found matches the way my avatar is stuck in a loop with the four animations it knows how to do. I live-performed changing the cameras and making my character jump to the sound, trying to create a bit of a sense of rhythm. The video is definitely half-baked. First of all, the environment is almost empty, so it’s almost impossible to tell that the character is moving through space. The wide shot doesn’t really work, and I think I should have significantly more close-ups of different body parts to loop through. But there might be some things in it that I could see taking further.&lt;/p&gt;

&lt;p&gt;My main question for the week is how I could edit with sound in the Sequencer, but I think the answer might just be a Google search away :).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">For this week’s assignment, I continued working with the Daz avatar I had created during the first week. I ended up practicing the workflow I discovered last week around mapping a Mixamo animation onto a Daz character’s skeleton, by bringing in 3 more animations for my character into Unreal.</summary></entry><entry><title type="html">[Performative Avatars] Skeletal Mesh</title><link href="http://localhost:4000/2020/09/22/skeletal-mesh.html" rel="alternate" type="text/html" title="[Performative Avatars] Skeletal Mesh" /><published>2020-09-22T01:01:00-04:00</published><updated>2020-09-22T01:01:00-04:00</updated><id>http://localhost:4000/2020/09/22/skeletal-mesh</id><content type="html" xml:base="http://localhost:4000/2020/09/22/skeletal-mesh.html">&lt;figure style=&quot;margin: 0; margin-bottom: 20px;&quot;&gt;
    &lt;video class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-22-skeletal-mesh/running.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
      &lt;figcaption&gt;
    Skeletal Mesh running around in my environment
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;For this week’s assignment, I revisited my understanding of skeletal meshes and working with them in Unreal Engine, through Matt’s tutorials. I became familiar with this type of mesh last semester, through work I did for Synthetic Architectures and for my Live Image Processing &amp;amp; Performance final project.&lt;/p&gt;

&lt;p&gt;The Unreal concepts are clear to me so far, I’m finally starting to feel a little bit of control over the software. My personal learning goal for this week however has been figuring out how to bring an avatar created in Daz Studio into Unreal, and applying a Mixamo animation to it. Bringing an FBX file exported from Daz into the Mixamo web interface doesn’t work properly (conversion fails, for some reason…,) so I had to look for alternatives. I found a &lt;a href=&quot;https://www.youtube.com/watch?v=pEK7fvAIV_A&quot;&gt;Youtube tutorial&lt;/a&gt; which explains how to use Maya as a bridge, in order to apply an animation downloaded from Mixamo to a custom Maya character – which in my case was a Daz character, transferred to Maya using the Daz to Maya bridge. This process involves creating a control rig for the downloaded Mixamo character, whose bone mapping needs to 100% match the control rig of the exported Daz character. I accidentally mis-mapped some of the hand bones (image below), which caused my character’s fingers to look completely broken, but other than that, the tutorial got me what I needed.&lt;/p&gt;

&lt;figure style=&quot;margin: 0; margin-bottom: 20px;&quot;&gt;
    &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-22-skeletal-mesh/hands.png&quot; /&gt;
  &lt;figcaption&gt;
    Twisted fingers because of hand skeleton wrong mapping in Maya
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The other thing I wanted to learn more about this week had to do with the textures associated with my character, so I gave him a few face tattoos (“Trust your body” / “Trust nobody”) by adding the text in Photoshop on the texture file associated with the body material.&lt;/p&gt;

&lt;p&gt;In the video above, I also used a camera shake in order to simulate the up-down running motion (camera is attached to character’s head, which can be seen in a few seconds at the beginning / end.) I also adjusted the character’s morph targets through a Blueprint. The animation would still need a good amount of work in order to feel somewhat realistic (running motion, speed, face gestures, mouth, etc.,) but I’m happy I got the Daz character to work with Mixamo animations.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">Skeletal Mesh running around in my environment For this week’s assignment, I revisited my understanding of skeletal meshes and working with them in Unreal Engine, through Matt’s tutorials. I became familiar with this type of mesh last semester, through work I did for Synthetic Architectures and for my Live Image Processing &amp;amp; Performance final project.</summary></entry><entry><title type="html">[Performative Avatars] Two Avatars</title><link href="http://localhost:4000/2020/09/14/avatar-creation.html" rel="alternate" type="text/html" title="[Performative Avatars] Two Avatars" /><published>2020-09-14T01:01:00-04:00</published><updated>2020-09-14T01:01:00-04:00</updated><id>http://localhost:4000/2020/09/14/avatar-creation</id><content type="html" xml:base="http://localhost:4000/2020/09/14/avatar-creation.html">&lt;figure style=&quot;&quot;&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/8.png&quot; alt=&quot;Daz Studio Avatar&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;I chose to build my two avatars in Second Life and Daz Studio. I had created digital versions of myself in Sims and Memoji over the summer, both of them being quite pleasant experiences. I can’t say the same about the two I made this week, even though it got me thinking about representation and about the peculiarities of my face.&lt;/p&gt;

&lt;p&gt;Choosing Second Life came out of pure curiosity – I had been aware of the platform’s existence and history for many years, and wanted to try it. It was a bad experience overall, mostly because of the software’s user interface and poor performance on my computer. The avatar creation system didn’t seem lacking in options when compared to other ones I’ve tried (plenty of options to customize the face &amp;amp; body structure,) but I had trouble creating a face that looked like mine. Strangely, the avatar height was somewhere at the top in the user interface – almost as the main defining characteristic of your image –, and for some reason I found myself caring a lot about it being accurate, 1.79m (5ft11). The hair options were incredibly limited, I had trouble finding something that made sense. The clothing system seemed to be almost entirely broken – I could not get shirt &amp;amp; pants option to load, and when I finally decided to rely on the default “Greg” pack I was given, I realized the clothes did not properly fit the mesh of my character.&lt;/p&gt;

&lt;figure style=&quot;margin: 0; margin-bottom: 20px;&quot;&gt;
    &lt;video class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/4.mp4&quot; muted=&quot;&quot; controls=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
      &lt;figcaption&gt;
    Second Life Timelapse
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/1.png&quot; alt=&quot;Second Life Avatar&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/2.png&quot; alt=&quot;Second Life Avatar&quot; /&gt;
  &lt;img class=&quot;img-row-3&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/3.png&quot; alt=&quot;Daz Studio Avatar&quot; /&gt;
  &lt;figcaption&gt;
    Second Life Avatars
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Building the second avatar in Daz Studio was a more pragmatic choice, as I had been curious to learn the basics of the software for some time. After watching a few Youtube tutorials in order to understand the interface, I started working with the Genesis 8 Male model (gender binaries are strong in this software and community…) I quickly realized I have limited control over my avatar with the basic settings, and ended up purchasing the Face Morphs for Genesis 8 Male pack, for $11. The purchase was worth its money – I got access to hundreds of sliders, allowing me to customize bones and muscles of my face I didn’t even know I had. It’s a similar system to the Second Life one, at a significantly higher quality and fidelity.&lt;/p&gt;

&lt;p&gt;This avatar doesn’t look like me either, even though parts of it probably do (the lips, the chin, maybe the eyebrows and the nose.) If anything, it looks like an older version of myself. With so much control over the face parameters of the avatar, I had to turn on my webcam and really study my own face, as an image or object, in a way that was new and unsettling. I started obsessing about the shape, placement or angle of various parts of my face, and stopped before spiraling down a dangerous rabbit hole :).&lt;/p&gt;

&lt;figure style=&quot;margin: 0; margin-bottom: 20px;&quot;&gt;
    &lt;video class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/5.mp4&quot; muted=&quot;&quot; controls=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
      &lt;figcaption&gt;
    Daz Studio Timelapse
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/6.png&quot; alt=&quot;Daz Studio Avatar&quot; /&gt;
  &lt;figcaption&gt;
    Daz Studio Avatar
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I also tried the Face Transfer feature of the software, which allows you to upload an image of yourself, and creates an avatar based on that. The result was absolutely terrifying (granted, I did not tweak any parameters or colors in the end result. It could probably be a bit better.)&lt;/p&gt;

&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-09-14-avatar-creation/7.png&quot; alt=&quot;Face Transfer Avatar&quot; /&gt;
  &lt;figcaption&gt;
  Face Transfer Avatar
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Performative Avatars" /><category term="ITP" /><summary type="html">I chose to build my two avatars in Second Life and Daz Studio. I had created digital versions of myself in Sims and Memoji over the summer, both of them being quite pleasant experiences. I can’t say the same about the two I made this week, even though it got me thinking about representation and about the peculiarities of my face.</summary></entry><entry><title type="html">[Homemade Hardware] Final Project Eagle Designs</title><link href="http://localhost:4000/2020/04/17/hh-eagle-designs.html" rel="alternate" type="text/html" title="[Homemade Hardware] Final Project Eagle Designs" /><published>2020-04-17T01:01:00-04:00</published><updated>2020-04-17T01:01:00-04:00</updated><id>http://localhost:4000/2020/04/17/hh-eagle-designs</id><content type="html" xml:base="http://localhost:4000/2020/04/17/hh-eagle-designs.html">&lt;p&gt;Bill of materials can be found &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1Uv07Ad9jPWgMuVOinqs0TzJjYUghBkELgq30I0iqyWk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.
You can also download the &lt;a href=&quot;/assets/media/blog/final-board.sch&quot;&gt;Eagle Schematic&lt;/a&gt; and the &lt;a href=&quot;/assets/media/blog/final-board.brd&quot;&gt;Eagle Board&lt;/a&gt;, as well as &lt;a href=&quot;/assets/media/blog/final-circuit.pdf&quot;&gt;PDF&lt;/a&gt; &lt;a href=&quot;/assets/media/blog/final-board.pdf&quot;&gt;versions&lt;/a&gt; of the two.&lt;/p&gt;
&lt;figure style=&quot;margin: 0;&quot;&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/media/blog/final-shot.png&quot; alt=&quot;Board design.&quot; /&gt;
  &lt;figcaption&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Homemade Hardware" /><category term="ITP" /><summary type="html">Bill of materials can be found here. You can also download the Eagle Schematic and the Eagle Board, as well as PDF versions of the two.</summary></entry><entry><title type="html">[Critical Communications] FCC License Geo-Research</title><link href="http://localhost:4000/2020/04/12/fcc-licenses.html" rel="alternate" type="text/html" title="[Critical Communications] FCC License Geo-Research" /><published>2020-04-12T01:01:00-04:00</published><updated>2020-04-12T01:01:00-04:00</updated><id>http://localhost:4000/2020/04/12/fcc-licenses</id><content type="html" xml:base="http://localhost:4000/2020/04/12/fcc-licenses.html">&lt;p&gt;For the second Critical Communications assignment, I used &lt;a href=&quot;https://www.radioreference.com&quot;&gt;Radio Reference&lt;/a&gt; in order to search for FCC radio licenses in a few different areas and industries. Here are some of my findings.&lt;/p&gt;
&lt;figure&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/media/blog/radio-screenshot.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Search #1: My neighborhood (zipcode 11221)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I did a 1 mile radius search around my home, using the &lt;a href=&quot;https://www.radioreference.com/apps/db/?action=fccProx&amp;amp;lat=40.6947392&amp;amp;lon=-73.9270738&amp;amp;r=1&quot;&gt;lat/lon FCC Callsign search tool&lt;/a&gt; that Radio Reference offers. I got about 150 results. While many of them belong to the city (NYPD, FDNY or simply City of New York communications,) there were lots of private license holders as well.&lt;/p&gt;

&lt;p&gt;The largest category of private license holders is taxi / car service companies (e.g. &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WPAX216&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WNIM264&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WQCS994&quot;&gt;3&lt;/a&gt;, &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WQVZ377&quot;&gt;4&lt;/a&gt;). They use their licenses for radio communications between cars, the traditional way taxi drivers talk to their dispatches and to each other. However, this result took me by surprise, as I rarely see taxis in the neighborhood. Most of these FCC licenses are still active, but the companies are either very small (1-2 person,) or have been put out of business by ride sharing apps over the past few years.&lt;/p&gt;

&lt;p&gt;I also found lots of licenses for NEXTEL OF NEW YORK. I had never heard of them, but in the meanwhile I learned they are an older telco provider, which is now (part of) Sprint. (?)&lt;/p&gt;

&lt;p&gt;Another significant category of license holders is schools (mostly charter schools). I don’t fully understand how they use radio communications, but it might have to do with synchronized clocks (see notes of &lt;a href=&quot;https://www.radioreference.com/apps/db/?fccCallsign=WQFN837&quot;&gt;Achievement First Bushwick Charter School&lt;/a&gt;.) Which leads me to the next discovery – &lt;a href=&quot;https://www.american-time.com/&quot;&gt;American Time&lt;/a&gt;. I had never heard of this company before – they offer synchronized time services to businesses. It’s unclear to me how this still exists when we have such widespread internet access, but I’ll let them be. One of their offerings is a &lt;a href=&quot;https://www.american-time.com/products-by-family/wall-clocks/battery-wall-clocks/radio-controlled-atomic-molded-case-clocks-fully-loaded&quot;&gt;radio controlled clock&lt;/a&gt;, which, according to the website, recives a signal over radio from an atomic clock in Colorado a few times a day in order to keep time precise. American Time holds licenses for a few thousands antennas (over 8,000 entries on their &lt;a href=&quot;https://www.radioreference.com/apps/db/?frn=0015495773&amp;amp;os=17000&amp;amp;s=ent&quot;&gt;Radio Reference page&lt;/a&gt;,) probably all used to keep their clocks all around America in sync.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Search #2: Menlo Park, Palo Alto, large tech companies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I was curious what the FCC radio license landscape is like in Silicon Valley, and how it’s different from the part of New York I live in. The radius searches in the area returned mostly 1) construction companies, 2) bio-tech companies and 3) tech companies. I started searching for company specific licenses for a few larger tech companies: Google, Facebook, Square, Palantir, Twitter, Airbnb, Pinterest, Tesla. Most of these had licenses for antennas located at their headquarters, in San Francisco / SV, or New York. Tesla also had some for their factories. Palantir didn’t have any (lol.) Google and Facebook had more than the other companies. I went deeper down the Facebook route, and realized that they have licenses for towers at the locations of their data centers as well. Which makes sense. However, the dystopian discovery that came out of this was that they got to name the streets where the data centers were built… &lt;a href=&quot;https://www.google.com/maps/place/Facebook+Data+Center/@32.9862414,-97.2594572,1279m/data=!3m1!1e3!4m13!1m7!3m6!1s0x864dd09915953d8d:0x97b577da8c062453!2s4500+Like+Way,+Fort+Worth,+TX+76177!3b1!8m2!3d32.984464!4d-97.2584655!3m4!1s0x864dd1dcb16993b1:0x1e2d97b188699b0!8m2!3d32.9867349!4d-97.2526476&quot;&gt;5000 Like Way, Fort Worth TX&lt;/a&gt; and &lt;a href=&quot;https://www.google.com/maps/place/Facebook+Data+Center/@41.6620952,-93.513984,1412m/data=!3m1!1e3!4m13!1m7!3m6!1s0x87ee917946eb868f:0x226cfeb7aacb03c!2s100+Share+Way+NW,+Altoona,+IA+50009!3b1!8m2!3d41.6632643!4d-93.5100769!3m4!1s0x87ee9176e99c041d:0x6035fff71a76b700!8m2!3d41.667416!4d-93.5076723&quot;&gt;100 Share Way NW, Altoona IA&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Search #3: Area 51 (Homey Airport)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;No results :).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using the &lt;a href=&quot;http://websdr.ewi.utwente.nl:8901/&quot;&gt;online SDR located at the Twente University&lt;/a&gt;, I caught a very clear recording of data being sent over the 26148.2kHz frequency, and an adjacent one (the one depicted in the screenshot). It looked like the two frequencies could be in dialogue, their messages never overlapped and alternated pretty consistently. Any ideas what it might be?&lt;/p&gt;

&lt;audio controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/media/blog/radio_holland_26148.2kHz.wav&quot; /&gt;
&lt;/audio&gt;

&lt;figure&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/media/blog/radio-screenshot-2.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/media/blog/radio-screenshot.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Critical Communications" /><category term="ITP" /><summary type="html">For the second Critical Communications assignment, I used Radio Reference in order to search for FCC radio licenses in a few different areas and industries. Here are some of my findings.</summary></entry><entry><title type="html">[Critical Communications] FTP</title><link href="http://localhost:4000/2020/04/05/ftp.html" rel="alternate" type="text/html" title="[Critical Communications] FTP" /><published>2020-04-05T01:01:00-04:00</published><updated>2020-04-05T01:01:00-04:00</updated><id>http://localhost:4000/2020/04/05/ftp</id><content type="html" xml:base="http://localhost:4000/2020/04/05/ftp.html">&lt;p&gt;Upon doing a round of research for communication protocols in the wild, as well as a few personal interests (&lt;em&gt;FM radio&lt;/em&gt;, which I had done a project with last semester, &lt;em&gt;telnet&lt;/em&gt; which is so old but still around or the &lt;em&gt;Spanning Tree Protocol&lt;/em&gt;, an early network routing protocol developed by a woman pioneer of computer science, Radia Perlman) I ended choosing &lt;em&gt;FTP&lt;/em&gt;, the File Transfer Protocol. I chose it mostly for nostalgic reasons. When I started using the internet in the early 2000s, I used to encounter sites served over ftp:// relatively often. Their visibility (and numbers) have been diminishing since, to the point where it took me about a reasonable amount of research in order to simply find one today. As the internet evolves, FTP will likely remain relevant for historical purposes, but its practical uses will be completely obsolete.&lt;/p&gt;

&lt;p&gt;The creation of FTP dates back to 1971. Developed by Abhay Bhushan while he was a student at MIT, the initial version of the protocol was published as &lt;a href=&quot;https://tools.ietf.org/html/rfc114&quot;&gt;IETF Request For Comments (RFC) 114&lt;/a&gt; and pre-dates TCP/IP (!!!). This proposal describes a mechanism for “indirect computer usage over a network” – an abstraction which allows a user to interact with a remote computer over the network without needing to log into the remote host, or be familiar with its command line interface. A number of RFCs improved upon the initial specification over the next few years (RFC 172, RFC 265, RFC 542, etc.,) all assuming the operation of this protocol on top of a TCP predecessor, NCP. In June 1980, after almost 10 years of existence, the protocol takes the shape of &lt;a href=&quot;https://tools.ietf.org/html/rfc765&quot;&gt;RFC 765&lt;/a&gt;, File Transfer Protocol Specification, which describes FTP communication over TCP. Another five years later, the protocol was revised and extended through &lt;a href=&quot;https://tools.ietf.org/html/rfc989&quot;&gt;RFC 959&lt;/a&gt;, which still serves as the base FTP protocol spec. Further feature and security improvements have been published since. All to say that the development of this protocol has been going hand in hand with many other low-level protocols which comprise the internet as we know it today, under the same institutional umbrella – U.S. universities or research institutions, with funding from the government or military.&lt;/p&gt;

&lt;p&gt;From a technical point of view, the protocol sits at the Application Layer of the internet protocol suite, similarly to HTTP, DNS or SSH. This means the protocol builds on top of TCP and the guarantees it offers: reliable and ordered message delivery between IP connected machines. The main goals of the protocol are data retrieval and transfer, and the commands specified in RFC 959 reflect that: RETRIEVE (RETR) and STORE (STOR) are the first ones to be listed, followed by another 20 or so commands which can manipulate the server’s file system or offer information about the service.&lt;/p&gt;

&lt;p&gt;The RFC also describes the anatomy of FTP connection between two machines as involving two communication lines: a control connection, and a data connection. The control connection is used for sending commands between the two machines (usually on port 21.) Upon establishing the control connection, the two machines agree on another set of ports for the data connection, which is where file transfers actually occur. The control connection between the two computers stays open for as long as needed, which is one of the main differences from the HTTP protocol – HTTP connections open when requests are made, and close once the data is sent, over a single line of communication.&lt;/p&gt;

&lt;p&gt;Since the design dates almost 50 years back, FTP performs file transfers in plain text, making it vulnerable from a security point of view. While the protocol specifies a mechanism for authentication, the username and password are also communicated without employing any encryption mechanisms. Secure FTP extensions have been created in the mid-late 1990s order to address the security concern, the most important ones being FTPS (FTP over SSL, just like HTTPS) and SFTP (FTP over an SSH tunnel.)&lt;/p&gt;

&lt;p&gt;Since I started going down rabbit hole in order to hopefully find some forgotten corners of the internet, I started searching for active FTP sites. I learned that FTP search engines exist and were (and are) used in similar ways to regular WWW search engines. &lt;a href=&quot;https://www.mmnt.ru/int/&quot;&gt;MAMONT&lt;/a&gt; is one that still works, and it sent me to a &lt;a href=&quot;ftp://212.85.110.14/Pub1/&quot;&gt;lovely window into 2005&lt;/a&gt; and &lt;a href=&quot;ftp://ftp.lyx.org/pub/doc/faqs/&quot;&gt;the largest collection of FAQs I have seen in my entire life&lt;/a&gt;. It made me remember the early internet browsing experience, and made me sad that major internet browsers are considering &lt;a href=&quot;https://www.pixelstech.net/article/1566007822-Google-plans-to-deprecate-FTP-URL-support-in-Chrome&quot;&gt;dropping&lt;/a&gt; &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=85464&quot;&gt;support&lt;/a&gt; for FTP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Critical Communications" /><category term="ITP" /><summary type="html">Upon doing a round of research for communication protocols in the wild, as well as a few personal interests (FM radio, which I had done a project with last semester, telnet which is so old but still around or the Spanning Tree Protocol, an early network routing protocol developed by a woman pioneer of computer science, Radia Perlman) I ended choosing FTP, the File Transfer Protocol. I chose it mostly for nostalgic reasons. When I started using the internet in the early 2000s, I used to encounter sites served over ftp:// relatively often. Their visibility (and numbers) have been diminishing since, to the point where it took me about a reasonable amount of research in order to simply find one today. As the internet evolves, FTP will likely remain relevant for historical purposes, but its practical uses will be completely obsolete.</summary></entry><entry><title type="html">[Synthetic Architectures] Final Treatment</title><link href="http://localhost:4000/2020/03/30/synth-arch-treatment.html" rel="alternate" type="text/html" title="[Synthetic Architectures] Final Treatment" /><published>2020-03-30T01:01:00-04:00</published><updated>2020-03-30T01:01:00-04:00</updated><id>http://localhost:4000/2020/03/30/synth-arch-treatment</id><content type="html" xml:base="http://localhost:4000/2020/03/30/synth-arch-treatment.html">&lt;figure&gt;
  &lt;img class=&quot;&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-30-synth-arch-treatment/1.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Link to the final project treatment can be found &lt;a href=&quot;/assets/media/blog/2_Treatment_v2.pdf&quot; target=&quot;__blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="Synthetic Architectures" /><category term="ITP" /><summary type="html"></summary></entry><entry><title type="html">[Homemade Hardware] Prototype</title><link href="http://localhost:4000/2020/03/27/homemade-hardware-prototype.html" rel="alternate" type="text/html" title="[Homemade Hardware] Prototype" /><published>2020-03-27T01:01:00-04:00</published><updated>2020-03-27T01:01:00-04:00</updated><id>http://localhost:4000/2020/03/27/homemade-hardware-prototype</id><content type="html" xml:base="http://localhost:4000/2020/03/27/homemade-hardware-prototype.html">&lt;p&gt;In my &lt;a href=&quot;&quot;&gt;previous post&lt;/a&gt; I proposed two possible directions for the final project. One involved an interactive video frame on a 7” screen, and the other one was a virtual road trip using Google Street View imagery on a smaller, 2.8” screen. I chose the second direction due to lack of access to video equipment and delays in shipping electronic components during this period.&lt;/p&gt;
&lt;figure&gt;
    &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/3-output-8.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;I did a good amount of small display research on Adafruit, and ended up deciding to get the &lt;a href=&quot;https://www.adafruit.com/product/1947&quot;&gt;2.8” TFT with capacitive display shield for Arduino UNO&lt;/a&gt;, which includes a MicroSD card slot. However, Adafruit stopped their shipping operations during the COVID-19 crisis, so I went to Tinkersphere and ordered a &lt;a href=&quot;https://tinkersphere.com/shields/1782-28-tft-touch-shield-for-arduino-with-resistive-touch-screen.html&quot;&gt;similar shield&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The sheild’s specifications on the Tinkersphere website mention it is compatible with the Adafruit_TFTLCD library, but I was not able to get it working (even though they use the same ILI9341 display.) I also tried the Adafruit_ILI9341 library without any luck. After doing some online research, I realized the Tinkersphere shield is different from the Adafruit one – they actually built the &lt;a href=&quot;https://www.hackster.io/baqwas/mcufriend-2-4-tft-display-e3c815&quot;&gt;MCUFRIEND 2.4” TFT Display Shield&lt;/a&gt;, which uses different components from Adafruit’s version. Fortunately, they have an Arduino library, &lt;a href=&quot;https://github.com/prenticedavid/MCUFRIEND_kbv&quot;&gt;MCUFRIEND_kbv&lt;/a&gt;, got the display showing the test graphics and eventually images from my MicroSD card after some tinkering.&lt;/p&gt;

&lt;figure&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/1.JPG&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
    Google Street View image from the I-93 highway displayed on the TFT screen, from MicroSD card.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In terms of software and gathering all the Street View images for an entire trip, I built a script using the Google Maps APIs. At a high level, my script takes two GPS coordinates as an input (origin and destination), asks the API for the driving directions from origin to destination, parses the directions and decodes the polyline for each segment of the route, determines the viewing angle between each two consecutive coordinates on the path and then asks the Street View API for an image at each coordinate, looking in the correct direction of the road.&lt;/p&gt;

&lt;p&gt;Here you can see results from two queries – one of them a short trip in New York, the other a longer trip in the desert – at different display frame rates (computer rendering, not Arduino.)&lt;/p&gt;

&lt;figure&gt;
    &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/2-output-12.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
    &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/2-output-8.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;

  &lt;figcaption&gt;  
  Left: 12fps; Right: 8fps;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/2-output-4.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
  &lt;/video&gt;
    &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/2-output-2.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
  &lt;figcaption&gt;  
  Left: 4fps; Right: 2fps;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/3-output-12.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
    &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/3-output-8.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;

  &lt;figcaption&gt;  
  Left: 12fps; Right: 8fps;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/3-output-6.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
  &lt;/video&gt;
    &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/3-output-2.mp4&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
    &lt;/video&gt;
  &lt;figcaption&gt;  
  Left: 6fps; Right: 2fps;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;An interesting artifact I hadn’t thought of before is the fact that the Street View car only travels in one direction for some road segments, so some of the footage will be backwards (the rear-view of the car, played in reverse.) The road motion is the same, but you can notice it in the cars passing by.&lt;/p&gt;

&lt;p&gt;I played the frames on Arduino as well. However, the display refresh rate is currently very slow: an image takes about 2.5 seconds to be loaded from the SD card and displayed on screen.&lt;/p&gt;

&lt;figure&gt;
  &lt;video class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-27-homemade-hardware-prototype/4.MOV&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot;&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;There’s something I really like about the very slow quality of the trip, but at the same time I would like to have more control over the frame rate, or the visual aspect of updating the screen. I’m currently using buffering, with a buffer size of ~80 pixels, which is about the maximum that will fit in Arduino’s 2kb memory. If I managed to get somewhere at 4-8 FPS that’d be great. But that means an image needs to be able to render in ~100-200ms, which is 20 times faster than my current time. Some ideas for optimizations include:&lt;/p&gt;

&lt;p&gt;– reducing the bit depth of my BMP files from 24 to 16 or 8. This would reduce the file sizes, therefore theoretically also reducing the read time of reading each frame from the MicroSD card.&lt;/p&gt;

&lt;p&gt;– lower image resolutions (currently 360x240), displayed on a portion of the screen. Same considerations as above, plus faster render times. This could work, given that I want to add text elements to the screen as well (compass direction, maybe city/town or coordinates.) However, anything less than 75% of the screen size would be too small.&lt;/p&gt;

&lt;p&gt;– replace the Arduino UNO with a board that has more SRAM. If I managed to fit one or two frames inside of the RAM, rendering could be significantly faster. However, I would need hundreds of kilobytes of SRAM per frame. Teensy boards have 64k, which would still allow me to have a more sizeable buffer.&lt;/p&gt;

&lt;p&gt;Another limitation for my initial idea is the API limit for Google Street View. Using the free Google Cloud Credits I’ve accumulated, I could make about 100,000 requests (1 image / request.) This will not be enough for the original NY – LA trip I had planned, but I can shorten my trip.
&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Homemade Hardware" /><category term="ITP" /><summary type="html">In my previous post I proposed two possible directions for the final project. One involved an interactive video frame on a 7” screen, and the other one was a virtual road trip using Google Street View imagery on a smaller, 2.8” screen. I chose the second direction due to lack of access to video equipment and delays in shipping electronic components during this period.</summary></entry><entry><title type="html">[Homemade Hardware] Final Project Proposal</title><link href="http://localhost:4000/2020/03/13/homemade-hardware-final.html" rel="alternate" type="text/html" title="[Homemade Hardware] Final Project Proposal" /><published>2020-03-13T01:01:00-04:00</published><updated>2020-03-13T01:01:00-04:00</updated><id>http://localhost:4000/2020/03/13/homemade-hardware-final</id><content type="html" xml:base="http://localhost:4000/2020/03/13/homemade-hardware-final.html">&lt;p&gt;Video presentation here, and text-based proposal below.&lt;/p&gt;
&lt;figure&gt;
  &lt;video style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-13-homemade-hardware-final/video.mov&quot; controls=&quot;&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  InfiniteObjects x Exonemo
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;During my first semester at ITP, I worked on two installations that involved interactive video. &lt;a href=&quot;/untitled&quot; target=&quot;__blank&quot;&gt;The first one&lt;/a&gt; was an overhead video feed of a person inside of a warehouse, shown on a computer monitor. The viewer had access to a computer mouse, and moving the mouse would make the person on screen walk towards the new position of the mouse.&lt;/p&gt;

&lt;figure&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-13-homemade-hardware-final/1.JPG&quot; alt=&quot;&quot; /&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-13-homemade-hardware-final/2.JPG&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  The viewer is able to control my position in the warehouse using the mouse. This is achieved by using ~1500 short video clips of walking between different positions in the space.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;/thejoys&quot; target=&quot;__blank&quot;&gt;The second one&lt;/a&gt; was slightly more elaborate, it involved 4 screens placed around the viewer, and a person stuck inside of the screen space. The person inside of the monitors would walk around the 4 screens (they are only present on one screen at a time,) making the viewer turn around in order to follow her. The viewer had access to a rotary phone and could call the on-screen person at any of the 4 screens.&lt;/p&gt;

&lt;figure&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-13-homemade-hardware-final/3.JPG&quot; alt=&quot;&quot; /&gt;
  &lt;img class=&quot;img-row-2&quot; style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-13-homemade-hardware-final/4.JPG&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the Homemade Hardware final project, I would love to continue working with video, potentially interactive depending on the technical constraints.&lt;/p&gt;

&lt;p&gt;A first direction I’m considering is inspired by &lt;a href=&quot;https://shop.infiniteobjects.com/collections/featured/products/exonemo-green&quot;&gt;Exonemo’s collaboration with Infinite Objects&lt;/a&gt;. I would build a video frame, on a 5” or 7” screen, in portrait mode, playing a short video of myself standing still, which loops perfectly. Whever the screen is touched (or a button is pressed,) I leave the frame for a few seconds, and then come back. Or, whenever the user touches the screen, a hand enters the frame and touches my face. Whenever the user lifts their finger, the hand leaves. There are a few content options I’m considering, but they all involve a very simple interaction (screen touch or button press) which triggers the video loop to move into a second state, and eventually get back to its original state. I think this idea would be effective if I can work with relatively high resolution / high FPS (30 or 60) video. Based on my research so far, this might be beyond the scope of the hardware we’ve been using in this class. However, I did find a few breakout boards on Adafruit that might do what I need, it’ll be a matter of figuring out how easy they are to rebuild.&lt;/p&gt;

&lt;figure&gt;
  &lt;video style=&quot;border: none;&quot; src=&quot;https://cdn.shopify.com/s/files/1/0021/7789/2441/files/exonemo-green-mobile.webm?2203&quot; muted=&quot;&quot; autoplay=&quot;&quot; loop=&quot;&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  InfiniteObjects x Exonemo
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;If that proves to be too difficult, I would be interested in working with different content on a small screen (&lt;a href=&quot;https://www.adafruit.com/product/2770&quot;&gt;2.8” TFT Display&lt;/a&gt; or variations, but definitely smaller than 3.5”.) I’ve always wanted to do a cross-country trip, but I don’t drive. This direction would involve using Google Street View and getting road images from New York to Los Angeles and back—at equal intervals, with as many images as I can fit on a MicroSD card. The final product would be a tiny device (the size of the screen, with some depth for the PCB) which slowly travels back and forth between New York and LA, by playing the road images in a sequence. I imagine being able to fit at least a few tens of thousands of images (at 320x240 resolution, probably even more.) The main constraints I’ve run into during my research for video with Arduino are low frame rates. However, for this direction, in the spirit of slowness, I would have a frame rate of &lt;em&gt;at most&lt;/em&gt; 1 FPS. If the screen updates its image line of pixels by line of pixels, that’s even better.&lt;/p&gt;

&lt;p&gt;Depending on the timeline, I could add a simple feature of displaying the lat,lon coordinates or the travel direction (N, S, E, W, like rear-view mirrors in some cars do) whenever the user taps the screen. These would all be saved on the microSD card. It would also be great to produce 3-4 of these objects, with different cities, but I realize that might not be realistic.&lt;/p&gt;

&lt;figure&gt;
  &lt;img style=&quot;border: none;&quot; src=&quot;/assets/images/blog/2020-03-13-homemade-hardware-final/5.png&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;  
  Google Street View image capture from New Jersey.
  &lt;/figcaption&gt;
&lt;/figure&gt;</content><author><name></name></author><category term="Homemade Hardware" /><category term="ITP" /><summary type="html">Video presentation here, and text-based proposal below. InfiniteObjects x Exonemo</summary></entry><entry><title type="html">[LIPP] First Performance</title><link href="http://localhost:4000/2020/02/26/first-performance.html" rel="alternate" type="text/html" title="[LIPP] First Performance" /><published>2020-02-26T00:01:00-05:00</published><updated>2020-02-26T00:01:00-05:00</updated><id>http://localhost:4000/2020/02/26/first-performance</id><content type="html" xml:base="http://localhost:4000/2020/02/26/first-performance.html">&lt;p&gt;My first performance was centered around the idea of politics as entertainment, It involves clips of the New Hampshire Democratic Debate, especially moments when the candidates breathe or make sounds with their bodies. This is a screen recording of the performance.&lt;/p&gt;

&lt;div class=&quot;mb2&quot; style=&quot;padding:59.41% 0 0 0;position:relative;background-color:black;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/395320823?autoplay=0&amp;amp;loop=0&amp;amp;title=0&amp;amp;byline=0&amp;amp;portrait=0&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</content><author><name></name></author><category term="LIPP" /><category term="ITP" /><summary type="html">My first performance was centered around the idea of politics as entertainment, It involves clips of the New Hampshire Democratic Debate, especially moments when the candidates breathe or make sounds with their bodies. This is a screen recording of the performance.</summary></entry></feed>